## ðŸ§  N-Gram Language Modeling and Evaluation

---------------------------

Author:
    Prakhar P. Tiwari | 121323248  
## ðŸ“˜ Overview
This project implements and evaluates **N-gram language models** on the Penn Treebank (PTB) dataset.  
It explores the effects of model order, smoothing, and backoff/interpolation strategies on **perplexity** and **text generation**.

The assignment demonstrates:
- Understanding of the **Markov assumption**
- Handling **data sparsity** via smoothing and interpolation
- Evaluating language models using **perplexity**
- Generating realistic text sequences from trained models

---

## ðŸ§© Features
âœ… Supports **1-gram to 4-gram** language models  
âœ… Implements:
- **Maximum Likelihood Estimation (MLE)**
- **Add-1 (Laplace) Smoothing**
- **Linear Interpolation** (Î»â‚â€“Î»â‚„)
- **Stupid Backoff** (Î± tuning)  
âœ… Automated **tuning** using random search with multithreading  
âœ… **Checkpointing** â€” skips retraining if models exist  
âœ… **CSV Logging** for perplexity results (`results/summary.csv`)  
âœ… **Text Generation** for both Interpolation and Backoff models  
âœ… Modular, reusable structure for experimentation

---

## ðŸ—ï¸ Project Structure

    Ngram_Language_Model/
    â”‚
    â”œâ”€â”€ data/
    â”‚ â”œâ”€â”€ ptb.train.txt
    â”‚ â”œâ”€â”€ ptb.valid.txt
    â”‚ â””â”€â”€ ptb.test.txt
    â”‚
    â”œâ”€â”€ src/
    â”‚ â”œâ”€â”€ preprocess.py
    â”‚ â”œâ”€â”€ ngram_model.py
    â”‚ â”œâ”€â”€ smoothing.py
    â”‚ â”œâ”€â”€ evaluate.py
    â”‚ â”œâ”€â”€ fine_tuning.py
    â”‚ â””â”€â”€ generate.py
    â”‚
    â”œâ”€â”€ models/
    â”‚ â”œâ”€â”€ uni.pkl
    â”‚ â”œâ”€â”€ bi.pkl
    â”‚ â”œâ”€â”€ tri.pkl
    â”‚ â”œâ”€â”€ tetra.pkl
    â”‚ â”œâ”€â”€ interp_best.pkl
    â”‚ â””â”€â”€ backoff_best.pkl
    â”‚
    â”œâ”€â”€ results/
    â”‚ â”œâ”€â”€ summary.csv
    â”‚ â”œâ”€â”€ generated_interpolation.txt
    â”‚ â””â”€â”€ generated_stupid_backoff.txt
    â”‚
    â”œâ”€â”€ main.py
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ Final_report.pdf
    â””â”€â”€ README.md


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Environment
```bash
conda create -n ngram_env python=3.10
conda activate ngram_env
pip install -r requirements.txt
```
ðŸš€ How to Run
â–¶ï¸ Training + Evaluation + Text Generation
python main.py

What Happens:

Loads PTB dataset and builds vocabulary
Trains N-gram models (1â€“4) if not already saved

Evaluates:

    1. MLE (unsmoothed)

    2. Add-1 smoothing

    3. Fine-tunes:

        Interpolation Î»â‚â€“Î»â‚„ (randomized + multithreading)

        Stupid Backoff Î± (parallel tuning)

    4. Logs all perplexities to results/summary.csv

    5. Generates 15 sentences from each model and saves them to:

        results/generated_interpolation.txt

        results/generated_stupid_backoff.txt

ðŸ“Š Example Console Output
[INFO] Base models found â€” loading instead of retraining.
[LOADED] uni.pkl ... tetra.pkl
[INFO] Evaluating unsmoothed models...
1-gram MLE perplexity: 948.22
2-gram MLE perplexity: 394.15
3-gram MLE perplexity: 244.17
4-gram MLE perplexity: 196.08

[INFO] Evaluating Add-1 (Laplace) smoothing...
...

[INFO] Tuning Î»â‚â€“Î»â‚„ using validation set...
Final Test Perplexity (Interpolation): 118.54
Final Test Perplexity (Stupid Backoff): 115.27
âœ… All evaluations and text generation complete.

ðŸ§¾ Output Files

    File	Description
    
    results/summary.csv ===>	Logs all model perplexities
    
    models/*.pkl ===>	Saved N-gram, Interpolation, and Backoff models
    
    results/generated_interpolation.txt	===> 15 sentences from interpolation model
    
    results/generated_stupid_backoff.txt ===> 15 sentences from backoff model


ðŸ§  Report Integration (Assignment Section 4)

 Final Result: Saved in Summary.py

 ![alt text](image.png)

 As we can see here We have achieved best model by stupid backoff with minimum perplexity of 128.82. The generated text from this model is saved in the results folder.